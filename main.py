#!/usr/bin/env python3
"""
å®Œæ•´çš„è«–æ–‡è™•ç†è…³æœ¬
æ•´åˆ arXiv çˆ¬èŸ²å’Œ AI æ‘˜è¦ç”ŸæˆåŠŸèƒ½
"""

import os
import sys
import json
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path

# æœ¬åœ°æ¨¡çµ„ - ä¿®æ­£å°å…¥è·¯å¾‘
from arxiv_crawler import ArxivCrawler
from ai_summarizer import AISummarizer

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def get_today_date():
    """å–å¾—ä»Šæ—¥æ—¥æœŸ"""
    custom_date = os.getenv('CUSTOM_DATE', '').strip()
    if custom_date:
        try:
            datetime.strptime(custom_date, '%Y-%m-%d')
            return custom_date
        except ValueError:
            logger.warning(f"âš ï¸ ç„¡æ•ˆçš„æ—¥æœŸæ ¼å¼: {custom_date}")
    
    # ä½¿ç”¨ timezone-aware datetime
    return datetime.now().strftime('%Y-%m-%d')

def generate_simple_summary(papers, target_date):
    """ç”Ÿæˆç°¡å–®çš„è«–æ–‡æ‘˜è¦ï¼ˆç„¡ AI å¢å¼·ï¼‰"""
    summary_lines = [
        f"# ğŸ“š æ¯æ—¥ ArXiv è«–æ–‡æ‘˜è¦",
        f"## ğŸ“… {target_date}",
        "",
        f"ä»Šæ—¥å…±æ‰¾åˆ° **{len(papers)}** ç¯‡è«–æ–‡",
        "",
        "---",
        ""
    ]
    
    for i, paper in enumerate(papers, 1):
        summary_lines.extend([
            f"### {i}. [{paper['title']}]({paper['arxiv_url']})",
            "",
            f"**ä½œè€…:** {', '.join(paper['authors'])}",
            f"**é¡åˆ¥:** {', '.join(paper['categories'])}",
            f"**ç™¼å¸ƒæ—¥æœŸ:** {paper['published'].strftime('%Y-%m-%d') if isinstance(paper['published'], datetime) else paper['published']}",
            "",
            f"[ğŸ“„ æŸ¥çœ‹è«–æ–‡]({paper['arxiv_url']}) | [ğŸ“¥ ä¸‹è¼‰ PDF]({paper['pdf_url']})",
            "",
            "**æ‘˜è¦:**",
            f"> {paper['summary'][:500]}{'...' if len(paper['summary']) > 500 else ''}",
            "",
            "---",
            ""
        ])
    
    summary_lines.extend([
        f"*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
    ])
    
    return '\n'.join(summary_lines)

def save_papers_data(papers, date_str):
    """å„²å­˜è«–æ–‡è³‡æ–™åˆ° JSON æª”æ¡ˆ"""
    try:
        data_dir = Path("data")
        data_dir.mkdir(exist_ok=True)
        
        # è½‰æ› datetime ç‰©ä»¶ç‚ºå­—ä¸²ä»¥ä¾¿ JSON åºåˆ—åŒ–
        papers_for_json = []
        for paper in papers:
            paper_copy = paper.copy()
            if isinstance(paper_copy.get('published'), datetime):
                paper_copy['published'] = paper['published'].isoformat()
            if isinstance(paper_copy.get('updated'), datetime):
                paper_copy['updated'] = paper['updated'].isoformat()
            papers_for_json.append(paper_copy)
        
        json_file = data_dir / f"{date_str}_papers.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(papers_for_json, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ è«–æ–‡è³‡æ–™å·²å„²å­˜: {json_file}")
        
    except Exception as e:
        logger.error(f"âŒ å„²å­˜è«–æ–‡è³‡æ–™å¤±æ•—: {e}")

def save_summary_report(summary, date_str):
    """å„²å­˜æ‘˜è¦å ±å‘Šåˆ° Markdown æª”æ¡ˆ"""
    try:
        data_dir = Path("data")
        data_dir.mkdir(exist_ok=True)
        
        report_file = data_dir / f"{date_str}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        logger.info(f"ğŸ“ æ‘˜è¦å ±å‘Šå·²å„²å­˜: {report_file}")
        return report_file
        
    except Exception as e:
        logger.error(f"âŒ å„²å­˜æ‘˜è¦å ±å‘Šå¤±æ•—: {e}")
        return None

def update_readme(reports_dir):
    """æ›´æ–° README æ–‡ä»¶"""
    try:
        if not reports_dir.exists():
            logger.warning("âš ï¸ data ç›®éŒ„ä¸å­˜åœ¨")
            return
        
        # æ‰¾åˆ°æ‰€æœ‰å ±å‘Šæ–‡ä»¶
        md_files = sorted(
            list(reports_dir.glob("*.md")),
            key=lambda x: x.stem,
            reverse=True
        )
        
        if not md_files:
            logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•å ±å‘Šæ–‡ä»¶")
            return
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ README ç¯„æœ¬
        template_path = Path("docs/readme_template.md")
        if template_path.exists():
            with open(template_path, 'r', encoding='utf-8') as f:
                template = f.read()
            
            # ç”Ÿæˆå ±å‘Šé€£çµ
            report_links = []
            for md_file in md_files[:15]:  # æœ€å¤šé¡¯ç¤º 15 å€‹
                date_str = md_file.stem
                link = f"- **{date_str}** ğŸ‘‰ [é»æ“ŠæŸ¥çœ‹å ±å‘Š](data/{md_file.name})"
                report_links.append(link)
            
            # æ›´æ–° README
            readme_content = template.format(
                report_list='\n'.join(report_links),
                last_update=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                total_reports=len(md_files)
            )
            
            with open("README.md", 'w', encoding='utf-8') as f:
                f.write(readme_content)
                
            logger.info("âœ… README.md æ›´æ–°å®Œæˆ")
        else:
            logger.info("â„¹ï¸ æ²’æœ‰æ‰¾åˆ° README ç¯„æœ¬ï¼Œè·³éæ›´æ–°")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–° README æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def check_environment():
    """æª¢æŸ¥ç’°å¢ƒè¨­å®š"""
    logger.info("ğŸ” æª¢æŸ¥ç’°å¢ƒè¨­å®š...")
    
    issues = []
    
    # æª¢æŸ¥ API é‡‘é‘°
    api_key = os.getenv('GOOGLE_API_KEY')
    if not api_key:
        issues.append("âŒ GOOGLE_API_KEY æœªè¨­å®š")
    else:
        logger.info("âœ… GOOGLE_API_KEY å·²è¨­å®š")
    
    # æª¢æŸ¥è¨­å®šæª”
    config_file = Path("config/topics.yaml")
    if not config_file.exists():
        issues.append("âš ï¸ config/topics.yaml ä¸å­˜åœ¨ï¼Œå°‡ä½¿ç”¨é è¨­è¨­å®š")
    else:
        logger.info("âœ… topics.yaml è¨­å®šæª”å­˜åœ¨")
    
    # æª¢æŸ¥ç›®éŒ„çµæ§‹
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)
    logger.info("âœ… data ç›®éŒ„å·²æº–å‚™")
    
    if issues:
        for issue in issues:
            logger.warning(issue)
    
    return len(issues) == 0

def main():
    """ä¸»è¦å‡½æ•¸"""
    try:
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œæ¯æ—¥ ArXiv è«–æ–‡è™•ç†")
        
        # æª¢æŸ¥ç’°å¢ƒ
        env_ok = check_environment()
        
        # å–å¾—ç›®æ¨™æ—¥æœŸ
        target_date = get_today_date()
        logger.info(f"ğŸ“… è™•ç†æ—¥æœŸ: {target_date}")
        
        # æª¢æŸ¥æ˜¯å¦å¼·åˆ¶æ›´æ–°
        force_update = os.getenv('FORCE_UPDATE', '').lower() == 'true'
        
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰ä»Šæ—¥çš„å ±å‘Š
        data_dir = Path("data")
        existing_report = data_dir / f"{target_date}.md"
        if existing_report.exists() and not force_update:
            logger.info(f"ğŸ“„ ä»Šæ—¥å ±å‘Šå·²å­˜åœ¨: {existing_report}")
            logger.info("ğŸ’¡ ä½¿ç”¨ FORCE_UPDATE=true ä¾†å¼·åˆ¶é‡æ–°ç”Ÿæˆ")
            return
        
        # åˆå§‹åŒ–çˆ¬èŸ²
        logger.info("ğŸ•·ï¸ åˆå§‹åŒ– ArXiv çˆ¬èŸ²...")
        crawler = ArxivCrawler()
        
        # çˆ¬å–è«–æ–‡
        logger.info("ğŸ“¡ é–‹å§‹çˆ¬å–è«–æ–‡...")
        papers = crawler.get_papers(target_date)
        
        if not papers:
            logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•è«–æ–‡")
            # ä»ç„¶ç”Ÿæˆä¸€å€‹ç©ºå ±å‘Š
            try:
                summarizer = AISummarizer()
                summary = summarizer._generate_empty_summary()
                save_summary_report(summary, target_date)
                update_readme(data_dir)
                logger.info("ğŸ“ å·²ç”Ÿæˆç©ºå ±å‘Š")
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆç©ºå ±å‘Šå¤±æ•—: {e}")
                # æ‰‹å‹•ç”Ÿæˆç©ºå ±å‘Š
                empty_summary = f"""# ğŸ“š æ¯æ—¥ ArXiv è«–æ–‡æ‘˜è¦

## ğŸ“… {target_date}

### ğŸ“­ ä»Šæ—¥ç„¡æ–°è«–æ–‡

ä»Šå¤©æ²’æœ‰ç™¼ç¾ç¬¦åˆæ¢ä»¶çš„æ–°è«–æ–‡ã€‚é€™å¯èƒ½æ˜¯ç”±æ–¼ï¼š

1. æ‰€é¸é¡åˆ¥ä»Šæ—¥æ²’æœ‰æ–°è«–æ–‡ç™¼å¸ƒ
2. ç¶²è·¯é€£æ¥å•é¡Œ
3. arXiv API æš«æ™‚ä¸å¯ç”¨

è«‹ç¨å¾Œå†ä¾†æŸ¥çœ‹ï¼

---

*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
                save_summary_report(empty_summary, target_date)
                update_readme(data_dir)
                logger.info("ğŸ“ å·²ç”Ÿæˆç°¡å–®ç©ºå ±å‘Š")
            return
        
        logger.info(f"ğŸ“Š æˆåŠŸç²å– {len(papers)} ç¯‡è«–æ–‡")
        
        # é¡¯ç¤ºé¡åˆ¥çµ±è¨ˆ
        try:
            category_stats = crawler.get_paper_categories_stats(papers)
            logger.info("ğŸ“Š è«–æ–‡é¡åˆ¥çµ±è¨ˆ:")
            for category, count in list(category_stats.items())[:5]:
                logger.info(f"   {category}: {count} ç¯‡")
        except Exception as e:
            logger.warning(f"âš ï¸ ç„¡æ³•é¡¯ç¤ºé¡åˆ¥çµ±è¨ˆ: {e}")
        
        # å„²å­˜åŸå§‹è«–æ–‡è³‡æ–™
        save_papers_data(papers, target_date)
        
        # åˆå§‹åŒ– AI æ‘˜è¦ç”Ÿæˆå™¨
        logger.info("ğŸ¤– åˆå§‹åŒ– AI æ‘˜è¦ç”Ÿæˆå™¨...")
        try:
            summarizer = AISummarizer()
            logger.info("âœ… AI æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ AI æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
            # ç”Ÿæˆä¸å¸¶ AI æ‘˜è¦çš„å ±å‘Š
            summary = generate_simple_summary(papers, target_date)
            report_file = save_summary_report(summary, target_date)
            if report_file:
                update_readme(data_dir)
                logger.info(f"ğŸ‰ ç”Ÿæˆäº†ç°¡å–®æ‘˜è¦å ±å‘Šï¼ˆç„¡ AI å¢å¼·ï¼‰")
            return
        
        # ç”Ÿæˆæ‘˜è¦
        logger.info("âœï¸ é–‹å§‹ç”Ÿæˆæ‘˜è¦...")
        try:
            summary = summarizer.generate_summary(papers)
        except Exception as e:
            logger.error(f"âŒ AI æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
            # ç”Ÿæˆç°¡å–®æ‘˜è¦
            summary = generate_simple_summary(papers, target_date)
        
        # å„²å­˜æ‘˜è¦å ±å‘Š
        report_file = save_summary_report(summary, target_date)
        
        if report_file:
            # æ›´æ–° README
            update_readme(data_dir)
            
            logger.info(f"ğŸ‰ è™•ç†å®Œæˆï¼ç”Ÿæˆäº†åŒ…å« {len(papers)} ç¯‡è«–æ–‡çš„æ‘˜è¦å ±å‘Š")
            logger.info(f"ğŸ“ å ±å‘Šæª”æ¡ˆ: {report_file}")
        else:
            logger.error("âŒ å„²å­˜å ±å‘Šå¤±æ•—")
            sys.exit(1)
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ ä½¿ç”¨è€…ä¸­æ–·ç¨‹å¼")
        sys.exit(0)
    except Exception as e:
        logger.error(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        logger.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()
