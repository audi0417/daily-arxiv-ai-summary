#!/usr/bin/env python3
"""
å®Œæ•´çš„è«–æ–‡è™•ç†è…³æœ¬
æ•´åˆ arXiv çˆ¬èŸ²å’Œ AI æ‘˜è¦ç”ŸæˆåŠŸèƒ½
"""

import os
import sys
import json
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ  src ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# æœ¬åœ°æ¨¡çµ„
from src.crawler import ArxivCrawler
from src.ai import AISummarizer

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def get_today_date():
    """å–å¾—ä»Šæ—¥æ—¥æœŸ"""
    custom_date = os.getenv('CUSTOM_DATE', '').strip()
    if custom_date:
        try:
            datetime.strptime(custom_date, '%Y-%m-%d')
            return custom_date
        except ValueError:
            logger.warning(f"âš ï¸ ç„¡æ•ˆçš„æ—¥æœŸæ ¼å¼: {custom_date}")
    
    return datetime.utcnow().strftime('%Y-%m-%d')

def save_papers_data(papers, date_str):
    """å„²å­˜è«–æ–‡è³‡æ–™åˆ° JSON æª”æ¡ˆ"""
    try:
        data_dir = Path("data")
        data_dir.mkdir(exist_ok=True)
        
        # è½‰æ› datetime ç‰©ä»¶ç‚ºå­—ä¸²ä»¥ä¾¿ JSON åºåˆ—åŒ–
        papers_for_json = []
        for paper in papers:
            paper_copy = paper.copy()
            paper_copy['published'] = paper['published'].isoformat()
            paper_copy['updated'] = paper['updated'].isoformat()
            papers_for_json.append(paper_copy)
        
        json_file = data_dir / f"{date_str}_papers.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(papers_for_json, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ è«–æ–‡è³‡æ–™å·²å„²å­˜: {json_file}")
        
    except Exception as e:
        logger.error(f"âŒ å„²å­˜è«–æ–‡è³‡æ–™å¤±æ•—: {e}")

def save_summary_report(summary, date_str):
    """å„²å­˜æ‘˜è¦å ±å‘Šåˆ° Markdown æª”æ¡ˆ"""
    try:
        data_dir = Path("data")
        data_dir.mkdir(exist_ok=True)
        
        report_file = data_dir / f"{date_str}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        logger.info(f"ğŸ“ æ‘˜è¦å ±å‘Šå·²å„²å­˜: {report_file}")
        return report_file
        
    except Exception as e:
        logger.error(f"âŒ å„²å­˜æ‘˜è¦å ±å‘Šå¤±æ•—: {e}")
        return None

def update_readme(reports_dir):
    """æ›´æ–° README æ–‡ä»¶"""
    try:
        if not reports_dir.exists():
            logger.warning("âš ï¸ data ç›®éŒ„ä¸å­˜åœ¨")
            return
        
        # æ‰¾åˆ°æ‰€æœ‰å ±å‘Šæ–‡ä»¶
        md_files = sorted(
            list(reports_dir.glob("*.md")),
            key=lambda x: x.stem,
            reverse=True
        )
        
        if not md_files:
            logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•å ±å‘Šæ–‡ä»¶")
            return
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ README ç¯„æœ¬
        template_path = Path("docs/readme_template.md")
        if template_path.exists():
            with open(template_path, 'r', encoding='utf-8') as f:
                template = f.read()
            
            # ç”Ÿæˆå ±å‘Šé€£çµ
            report_links = []
            for md_file in md_files[:15]:  # æœ€å¤šé¡¯ç¤º 15 å€‹
                date_str = md_file.stem
                link = f"- **{date_str}** ğŸ‘‰ [é»æ“ŠæŸ¥çœ‹å ±å‘Š](data/{md_file.name})"
                report_links.append(link)
            
            # æ›´æ–° README
            readme_content = template.format(
                report_list='\n'.join(report_links),
                last_update=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                total_reports=len(md_files)
            )
            
            with open("README.md", 'w', encoding='utf-8') as f:
                f.write(readme_content)
                
            logger.info("âœ… README.md æ›´æ–°å®Œæˆ")
        else:
            logger.info("â„¹ï¸ æ²’æœ‰æ‰¾åˆ° README ç¯„æœ¬ï¼Œè·³éæ›´æ–°")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–° README æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def check_environment():
    """æª¢æŸ¥ç’°å¢ƒè¨­å®š"""
    logger.info("ğŸ” æª¢æŸ¥ç’°å¢ƒè¨­å®š...")
    
    issues = []
    
    # æª¢æŸ¥ API é‡‘é‘°
    api_key = os.getenv('GOOGLE_API_KEY')
    if not api_key:
        issues.append("âŒ GOOGLE_API_KEY æœªè¨­å®š")
    else:
        logger.info("âœ… GOOGLE_API_KEY å·²è¨­å®š")
    
    # æª¢æŸ¥è¨­å®šæª”
    config_file = Path("config/topics.yaml")
    if not config_file.exists():
        issues.append("âš ï¸ config/topics.yaml ä¸å­˜åœ¨ï¼Œå°‡ä½¿ç”¨é è¨­è¨­å®š")
    else:
        logger.info("âœ… topics.yaml è¨­å®šæª”å­˜åœ¨")
    
    # æª¢æŸ¥ç›®éŒ„çµæ§‹
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)
    logger.info("âœ… data ç›®éŒ„å·²æº–å‚™")
    
    if issues:
        for issue in issues:
            logger.warning(issue)
    
    return len(issues) == 0

def main():
    """ä¸»è¦å‡½æ•¸"""
    try:
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œæ¯æ—¥ ArXiv è«–æ–‡è™•ç†")
        
        # æª¢æŸ¥ç’°å¢ƒ
        env_ok = check_environment()
        
        # å–å¾—ç›®æ¨™æ—¥æœŸ
        target_date = get_today_date()
        logger.info(f"ğŸ“… è™•ç†æ—¥æœŸ: {target_date}")
        
        # æª¢æŸ¥æ˜¯å¦å¼·åˆ¶æ›´æ–°
        force_update = os.getenv('FORCE_UPDATE', '').lower() == 'true'
        
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰ä»Šæ—¥çš„å ±å‘Š
        data_dir = Path("data")
        existing_report = data_dir / f"{target_date}.md"
        if existing_report.exists() and not force_update:
            logger.info(f"ğŸ“„ ä»Šæ—¥å ±å‘Šå·²å­˜åœ¨: {existing_report}")
            logger.info("ğŸ’¡ ä½¿ç”¨ FORCE_UPDATE=true ä¾†å¼·åˆ¶é‡æ–°ç”Ÿæˆ")
            return
        
        # åˆå§‹åŒ–çˆ¬èŸ²
        logger.info("ğŸ•·ï¸ åˆå§‹åŒ– ArXiv çˆ¬èŸ²...")
        crawler = ArxivCrawler()
        
        # çˆ¬å–è«–æ–‡
        logger.info("ğŸ“¡ é–‹å§‹çˆ¬å–è«–æ–‡...")
        papers = crawler.get_papers(target_date)
        
        if not papers:
            logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•è«–æ–‡")
            # ä»ç„¶ç”Ÿæˆä¸€å€‹ç©ºå ±å‘Š
            summarizer = AISummarizer()
            summary = summarizer._generate_empty_summary()
            save_summary_report(summary, target_date)
            update_readme(data_dir)
            logger.info("ğŸ“ å·²ç”Ÿæˆç©ºå ±å‘Š")
            return
        
        logger.info(f"ğŸ“Š æˆåŠŸç²å– {len(papers)} ç¯‡è«–æ–‡")
        
        # é¡¯ç¤ºé¡åˆ¥çµ±è¨ˆ
        category_stats = crawler.get_paper_categories_stats(papers)
        logger.info("ğŸ“Š è«–æ–‡é¡åˆ¥çµ±è¨ˆ:")
        for category, count in list(category_stats.items())[:5]:
            logger.info(f"   {category}: {count} ç¯‡")
        
        # å„²å­˜åŸå§‹è«–æ–‡è³‡æ–™
        save_papers_data(papers, target_date)
        
        # åˆå§‹åŒ– AI æ‘˜è¦ç”Ÿæˆå™¨
        logger.info("ğŸ¤– åˆå§‹åŒ– AI æ‘˜è¦ç”Ÿæˆå™¨...")
        summarizer = AISummarizer()
        
        # ç”Ÿæˆæ‘˜è¦
        logger.info("âœï¸ é–‹å§‹ç”Ÿæˆæ‘˜è¦...")
        summary = summarizer.generate_summary(papers)
        
        # å„²å­˜æ‘˜è¦å ±å‘Š
        report_file = save_summary_report(summary, target_date)
        
        if report_file:
            # æ›´æ–° README
            update_readme(data_dir)
            
            logger.info(f"ğŸ‰ è™•ç†å®Œæˆï¼ç”Ÿæˆäº†åŒ…å« {len(papers)} ç¯‡è«–æ–‡çš„æ‘˜è¦å ±å‘Š")
            logger.info(f"ğŸ“ å ±å‘Šæª”æ¡ˆ: {report_file}")
        else:
            logger.error("âŒ å„²å­˜å ±å‘Šå¤±æ•—")
            sys.exit(1)
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ ä½¿ç”¨è€…ä¸­æ–·ç¨‹å¼")
        sys.exit(0)
    except Exception as e:
        logger.error(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
