name: Test ArXiv Crawler Fix

on:
  workflow_dispatch:
    inputs:
      test_date:
        description: 'Test date (YYYY-MM-DD)'
        required: false
        default: '2025-06-11'
        type: string
      debug_mode:
        description: 'Enable debug mode'
        required: false
        default: true
        type: boolean

permissions:
  contents: write
  actions: read

jobs:
  test_crawler:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create test script
      run: |
        cat > test_crawler.py << 'EOF'
        import logging
        import sys
        import traceback
        import os
        
        # Setup logging
        logging.basicConfig(
            level=logging.DEBUG if os.getenv('DEBUG_MODE') == 'true' else logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        def main():
            try:
                print("Starting crawler test...")
                from arxiv_crawler import ArxivCrawler
                
                print("Crawler module imported successfully")
                crawler = ArxivCrawler()
                print("Crawler initialized successfully")
                
                test_date = os.getenv('TEST_DATE', '2025-06-11')
                print(f"Testing paper retrieval for date: {test_date}")
                
                papers = crawler.get_papers(test_date)
                print(f"Successfully retrieved {len(papers)} papers")
                
                if papers:
                    print("First paper info:")
                    paper = papers[0]
                    print(f"  Title: {paper.get('title', 'Unknown')}")
                    print(f"  Authors: {len(paper.get('authors', []))} authors")
                    print(f"  Categories: {paper.get('categories', [])}")
                    print(f"  ArXiv ID: {paper.get('arxiv_id', 'Unknown')}")
                    print(f"  URL: {paper.get('arxiv_url', 'Unknown')}")
                else:
                    print("No papers found. Possible reasons:")
                    print("  - Network connection issues")
                    print("  - ArXiv API temporarily unavailable")
                    print("  - Search criteria too restrictive")
                    
                return len(papers)
                
            except Exception as e:
                print(f"Error during testing: {e}")
                if os.getenv('DEBUG_MODE') == 'true':
                    print("Detailed error information:")
                    traceback.print_exc()
                return -1
        
        if __name__ == "__main__":
            result = main()
            if result >= 0:
                print(f"Test completed successfully. Found {result} papers.")
            else:
                print("Test failed.")
                sys.exit(1)
        EOF
        
    - name: Run crawler test
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        MODEL_NAME: ${{ vars.MODEL_NAME || 'gemini-2.0-flash-exp' }}
        LANGUAGE: ${{ vars.LANGUAGE || 'Traditional Chinese' }}
        TEST_DATE: ${{ github.event.inputs.test_date }}
        DEBUG_MODE: ${{ github.event.inputs.debug_mode }}
        FORCE_UPDATE: 'true'
      run: |
        echo "Running crawler test..."
        echo "Test date: ${{ github.event.inputs.test_date }}"
        echo "Debug mode: ${{ github.event.inputs.debug_mode }}"
        
        if [ "${{ github.event.inputs.debug_mode }}" = "true" ]; then
          export PYTHONUNBUFFERED=1
          python test_crawler.py
        else
          echo "Running full pipeline..."
          export CUSTOM_DATE="${{ github.event.inputs.test_date }}"
          python main.py
        fi
        
    - name: Generate test report
      if: always()
      run: |
        echo "## Test Results" >> $GITHUB_STEP_SUMMARY
        echo "- **Test Time**: $(date -u)" >> $GITHUB_STEP_SUMMARY
        echo "- **Test Date**: ${{ github.event.inputs.test_date }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Debug Mode**: ${{ github.event.inputs.debug_mode }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Check for generated files
        if [ -f "data/${{ github.event.inputs.test_date }}.md" ]; then
          echo "- **Report Generated**: ✅ Success" >> $GITHUB_STEP_SUMMARY
          file_size=$(wc -c < "data/${{ github.event.inputs.test_date }}.md")
          echo "- **Report Size**: ${file_size} bytes" >> $GITHUB_STEP_SUMMARY
        else
          echo "- **Report Generated**: ❌ Failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f "data/${{ github.event.inputs.test_date }}_papers.json" ]; then
          echo "- **Data File**: ✅ Exists" >> $GITHUB_STEP_SUMMARY
          paper_count=$(python -c "
        import json
        try:
            with open('data/${{ github.event.inputs.test_date }}_papers.json', 'r') as f:
                papers = json.load(f)
            print(len(papers))
        except:
            print(0)
        " 2>/dev/null || echo "0")
          echo "- **Paper Count**: ${paper_count} papers" >> $GITHUB_STEP_SUMMARY
        else
          echo "- **Data File**: ❌ Not found" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## System Status" >> $GITHUB_STEP_SUMMARY
        echo "- **Python Version**: $(python --version)" >> $GITHUB_STEP_SUMMARY
        echo "- **API Key**: ${{ secrets.GOOGLE_API_KEY && 'Configured' || 'Not configured' }}" >> $GITHUB_STEP_SUMMARY
        
    - name: Cleanup
      if: always()
      run: |
        rm -f test_crawler.py
