#!/usr/bin/env python3
"""
ArXiv è«–æ–‡çˆ¬èŸ²æ¨¡çµ„
ç”¨æ–¼å¾ arXiv API ç²å–æœ€æ–°è«–æ–‡è³‡è¨Š
"""

import os
import re
import time
import logging
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlencode
import yaml

logger = logging.getLogger(__name__)

class ArxivCrawler:
    """ArXiv è«–æ–‡çˆ¬èŸ²"""
    
    def __init__(self, config_path: str = "config/topics.yaml"):
        """
        åˆå§‹åŒ–çˆ¬èŸ²
        
        Args:
            config_path: è¨­å®šæª”è·¯å¾‘
        """
        self.base_url = "http://export.arxiv.org/api/query"
        self.config = self._load_config(config_path)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ArXiv-Daily-Summary/1.0 (https://github.com/audi0417/daily-arxiv-ai-summary)'
        })
        # è¨­ç½®é‡è©¦åƒæ•¸
        self.max_retries = 3
        self.retry_delay = 5
        
    def _load_config(self, config_path: str) -> Dict:
        """è¼‰å…¥è¨­å®šæª”"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥è¨­å®šæª”: {config_path}")
            return config
        except FileNotFoundError:
            logger.warning(f"âš ï¸ è¨­å®šæª”ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é è¨­è¨­å®š")
            return self._get_default_config()
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥è¨­å®šæª”å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """å–å¾—é è¨­è¨­å®š"""
        return {
            'categories': ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL', 'cs.NE', 'cs.IR', 'cs.HC', 'stat.ML', 'stat.AP', 'math.OC'],
            'limits': {
                'max_papers_per_day': 50,
                'max_papers_per_category': 10
            },
            'date_filter': {
                'recent_days': 3,
                'include_weekends': True
            },
            'keywords': {
                'include': [
                    'transformer', 'attention', 'deep learning', 
                    'neural network', 'machine learning', 'artificial intelligence'
                ]
            }
        }
    
    def _build_search_query(self, categories: List[str], date_from: str) -> str:
        """
        å»ºæ§‹ arXiv æœå°‹æŸ¥è©¢
        
        Args:
            categories: è«–æ–‡é¡åˆ¥åˆ—è¡¨
            date_from: èµ·å§‹æ—¥æœŸ (YYYYMMDD)
            
        Returns:
            æœå°‹æŸ¥è©¢å­—ä¸²
        """
        # å»ºæ§‹é¡åˆ¥æŸ¥è©¢
        cat_queries = [f"cat:{cat}" for cat in categories]
        cat_query = " OR ".join(cat_queries)
        
        # å»ºæ§‹æ—¥æœŸæŸ¥è©¢
        date_query = f"submittedDate:[{date_from}* TO *]"
        
        # çµåˆæŸ¥è©¢
        full_query = f"({cat_query}) AND {date_query}"
        
        logger.info(f"ğŸ” æœå°‹æŸ¥è©¢: {full_query}")
        return full_query
    
    def _parse_paper_entry(self, entry: ET.Element) -> Optional[Dict]:
        """
        è§£æå–®ç¯‡è«–æ–‡è³‡è¨Š
        
        Args:
            entry: XML entry å…ƒç´ 
            
        Returns:
            è«–æ–‡è³‡è¨Šå­—å…¸
        """
        try:
            # å®šç¾©å‘½åç©ºé–“
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            # åŸºæœ¬è³‡è¨Š
            id_elem = entry.find('atom:id', namespaces)
            if id_elem is None:
                return None
            
            paper_id = id_elem.text
            arxiv_id = paper_id.split('/')[-1]
            
            # æ¨™é¡Œ
            title_elem = entry.find('atom:title', namespaces)
            title = title_elem.text.strip() if title_elem is not None else "æœªçŸ¥æ¨™é¡Œ"
            title = re.sub(r'\s+', ' ', title)  # æ¸…ç†å¤šé¤˜ç©ºæ ¼
            
            # æ‘˜è¦
            summary_elem = entry.find('atom:summary', namespaces)
            summary = summary_elem.text.strip() if summary_elem is not None else "ç„¡æ‘˜è¦"
            summary = re.sub(r'\s+', ' ', summary)  # æ¸…ç†å¤šé¤˜ç©ºæ ¼
            
            # ä½œè€…è³‡è¨Š
            authors = []
            for author in entry.findall('atom:author', namespaces):
                name_elem = author.find('atom:name', namespaces)
                if name_elem is not None:
                    authors.append(name_elem.text)
            
            # é¡åˆ¥è³‡è¨Š
            categories = []
            
            # ä¸»è¦é¡åˆ¥
            primary_cat = entry.find('arxiv:primary_category', namespaces)
            if primary_cat is not None:
                categories.append(primary_cat.get('term'))
            
            # æ‰€æœ‰é¡åˆ¥
            for cat in entry.findall('atom:category', namespaces):
                cat_term = cat.get('term')
                if cat_term and cat_term not in categories:
                    categories.append(cat_term)
            
            # æ—¥æœŸè³‡è¨Š
            published_elem = entry.find('atom:published', namespaces)
            if published_elem is not None:
                published_raw = published_elem.text
                published = datetime.fromisoformat(published_raw.replace('Z', '+00:00'))
            else:
                published = datetime.now()
            
            updated_elem = entry.find('atom:updated', namespaces)
            if updated_elem is not None:
                updated_raw = updated_elem.text
                updated = datetime.fromisoformat(updated_raw.replace('Z', '+00:00'))
            else:
                updated = published
            
            # PDF é€£çµ
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è©•è«–
            comment_elem = entry.find('arxiv:comment', namespaces)
            comment = comment_elem.text if comment_elem is not None else None
            
            return {
                'arxiv_id': arxiv_id,
                'title': title,
                'authors': authors,
                'summary': summary,
                'categories': categories,
                'published': published,
                'updated': updated,
                'pdf_url': pdf_url,
                'arxiv_url': f"https://arxiv.org/abs/{arxiv_id}",
                'comment': comment
            }
            
        except Exception as e:
            logger.error(f"âŒ è§£æè«–æ–‡è³‡è¨Šå¤±æ•—: {e}")
            return None
    
    def _filter_papers_by_keywords(self, papers: List[Dict]) -> List[Dict]:
        """
        æ ¹æ“šé—œéµå­—éæ¿¾è«–æ–‡
        
        Args:
            papers: è«–æ–‡åˆ—è¡¨
            
        Returns:
            éæ¿¾å¾Œçš„è«–æ–‡åˆ—è¡¨
        """
        if 'keywords' not in self.config:
            return papers
        
        keywords_config = self.config['keywords']
        include_keywords = keywords_config.get('include', [])
        exclude_keywords = keywords_config.get('exclude', [])
        
        if not include_keywords and not exclude_keywords:
            return papers
        
        filtered_papers = []
        
        for paper in papers:
            text_to_search = f"{paper['title']} {paper['summary']}".lower()
            
            # æª¢æŸ¥åŒ…å«é—œéµå­—
            if include_keywords:
                has_include_keyword = any(
                    keyword.lower() in text_to_search 
                    for keyword in include_keywords
                )
                if not has_include_keyword:
                    continue
            
            # æª¢æŸ¥æ’é™¤é—œéµå­—
            if exclude_keywords:
                has_exclude_keyword = any(
                    keyword.lower() in text_to_search 
                    for keyword in exclude_keywords
                )
                if has_exclude_keyword:
                    continue
            
            filtered_papers.append(paper)
        
        logger.info(f"ğŸ” é—œéµå­—éæ¿¾: {len(papers)} â†’ {len(filtered_papers)}")
        return filtered_papers
    
    def _apply_limits(self, papers: List[Dict]) -> List[Dict]:
        """
        æ‡‰ç”¨è«–æ–‡æ•¸é‡é™åˆ¶
        
        Args:
            papers: è«–æ–‡åˆ—è¡¨
            
        Returns:
            é™åˆ¶å¾Œçš„è«–æ–‡åˆ—è¡¨
        """
        limits = self.config.get('limits', {})
        max_papers = limits.get('max_papers_per_day', 50)
        
        if len(papers) > max_papers:
            # æŒ‰ç™¼å¸ƒæ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
            papers_sorted = sorted(papers, key=lambda x: x['published'], reverse=True)
            papers = papers_sorted[:max_papers]
            logger.info(f"ğŸ“Š æ‡‰ç”¨è«–æ–‡æ•¸é‡é™åˆ¶: {max_papers}")
        
        return papers
    
    def get_papers(self, target_date: Optional[str] = None) -> List[Dict]:
        """
        ç²å–æŒ‡å®šæ—¥æœŸçš„è«–æ–‡
        
        Args:
            target_date: ç›®æ¨™æ—¥æœŸ (YYYY-MM-DD)ï¼Œé è¨­ç‚ºä»Šæ—¥
            
        Returns:
            è«–æ–‡åˆ—è¡¨
        """
        if target_date is None:
            target_date = datetime.utcnow().strftime('%Y-%m-%d')
        
        # è¨ˆç®—æœå°‹æ—¥æœŸç¯„åœ
        target_dt = datetime.strptime(target_date, '%Y-%m-%d')
        recent_days = self.config.get('date_filter', {}).get('recent_days', 3)
        start_date = target_dt - timedelta(days=recent_days)
        date_from = start_date.strftime('%Y%m%d')
        
        logger.info(f"ğŸ” æœå°‹æ—¥æœŸç¯„åœ: {start_date.strftime('%Y-%m-%d')} åˆ° {target_date}")
        
        # å–å¾—é¡åˆ¥åˆ—è¡¨
        categories = self.config.get('categories', ['cs.AI', 'cs.LG'])
        logger.info(f"ğŸ“š æœå°‹é¡åˆ¥: {categories}")
        
        # å»ºæ§‹æœå°‹æŸ¥è©¢
        search_query = self._build_search_query(categories, date_from)
        
        # åŸ·è¡Œæœå°‹
        papers = self._search_papers(search_query)
        
        if not papers:
            logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•è«–æ–‡")
            return []
        
        logger.info(f"ğŸ“„ æ‰¾åˆ° {len(papers)} ç¯‡è«–æ–‡")
        
        # æ‡‰ç”¨éæ¿¾æ¢ä»¶
        papers = self._filter_papers_by_keywords(papers)
        papers = self._apply_limits(papers)
        
        logger.info(f"âœ… æœ€çµ‚ç²å¾— {len(papers)} ç¯‡è«–æ–‡")
        return papers
    
    def _search_papers(self, query: str, max_results: int = 100) -> List[Dict]:
        """
        åŸ·è¡Œ arXiv æœå°‹ï¼Œæ”¯æŒé‡è©¦æ©Ÿåˆ¶
        
        Args:
            query: æœå°‹æŸ¥è©¢
            max_results: æœ€å¤§çµæœæ•¸é‡
            
        Returns:
            è«–æ–‡åˆ—è¡¨
        """
        params = {
            'search_query': query,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        url = f"{self.base_url}?{urlencode(params)}"
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f"ğŸŒ ç™¼é€è«‹æ±‚åˆ° arXiv API (å˜—è©¦ {attempt + 1}/{self.max_retries})...")
                
                # ä½¿ç”¨æ›´é•·çš„è¶…æ™‚æ™‚é–“
                response = self.session.get(url, timeout=60)
                response.raise_for_status()
                
                # æª¢æŸ¥å›æ‡‰æ˜¯å¦ç‚ºç©º
                if not response.content:
                    logger.warning("âš ï¸ æ”¶åˆ°ç©ºå›æ‡‰")
                    if attempt < self.max_retries - 1:
                        time.sleep(self.retry_delay)
                        continue
                    else:
                        return []
                
                # è§£æ XML å›æ‡‰
                try:
                    root = ET.fromstring(response.content)
                except ET.ParseError as e:
                    logger.error(f"âŒ XML è§£æå¤±æ•—: {e}")
                    if attempt < self.max_retries - 1:
                        time.sleep(self.retry_delay)
                        continue
                    else:
                        return []
                
                # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤è¨Šæ¯
                total_results_elem = root.find('.//{http://a9.com/-/spec/opensearch/1.1/}totalResults')
                if total_results_elem is not None:
                    total_results = int(total_results_elem.text)
                    logger.info(f"ğŸ“Š arXiv å›å ±ç¸½çµæœæ•¸: {total_results}")
                
                # è§£æè«–æ–‡æ¢ç›®
                papers = []
                namespaces = {
                    'atom': 'http://www.w3.org/2005/Atom',
                    'arxiv': 'http://arxiv.org/schemas/atom'
                }
                
                entries = root.findall('atom:entry', namespaces)
                logger.info(f"ğŸ“„ è§£æåˆ° {len(entries)} å€‹æ¢ç›®")
                
                for entry in entries:
                    paper = self._parse_paper_entry(entry)
                    if paper:
                        papers.append(paper)
                
                # API è«‹æ±‚é™åˆ¶ï¼šæ¯ 3 ç§’æœ€å¤š 1 æ¬¡è«‹æ±‚
                time.sleep(3)
                
                logger.info(f"âœ… æˆåŠŸè§£æ {len(papers)} ç¯‡è«–æ–‡")
                return papers
                
            except requests.exceptions.Timeout:
                logger.warning(f"âš ï¸ è«‹æ±‚è¶…æ™‚ (å˜—è©¦ {attempt + 1})")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    logger.error("âŒ æ‰€æœ‰é‡è©¦éƒ½è¶…æ™‚")
                    
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"âš ï¸ é€£æ¥éŒ¯èª¤ (å˜—è©¦ {attempt + 1}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    logger.error("âŒ æ‰€æœ‰é‡è©¦éƒ½é€£æ¥å¤±æ•—")
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    logger.error("âŒ æ‰€æœ‰ç¶²è·¯è«‹æ±‚éƒ½å¤±æ•—")
                    
            except Exception as e:
                logger.error(f"âŒ æœå°‹è«–æ–‡æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ (å˜—è©¦ {attempt + 1}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    logger.error("âŒ æ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—")
        
        return []
    
    def get_paper_categories_stats(self, papers: List[Dict]) -> Dict[str, int]:
        """
        å–å¾—è«–æ–‡é¡åˆ¥çµ±è¨ˆ
        
        Args:
            papers: è«–æ–‡åˆ—è¡¨
            
        Returns:
            é¡åˆ¥çµ±è¨ˆå­—å…¸
        """
        stats = {}
        for paper in papers:
            for category in paper['categories']:
                stats[category] = stats.get(category, 0) + 1
        
        return dict(sorted(stats.items(), key=lambda x: x[1], reverse=True))
