#!/usr/bin/env python3
"""
ArXiv è«–æ–‡çˆ¬èŸ²æ¨¡çµ„
ç”¨æ–¼å¾ arXiv API ç²å–æœ€æ–°è«–æ–‡è³‡è¨Š
"""

import os
import re
import time
import logging
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlencode
import yaml

logger = logging.getLogger(__name__)

class ArxivCrawler:
    """ArXiv è«–æ–‡çˆ¬èŸ²"""
    
    def __init__(self, config_path: str = "config/topics.yaml"):
        """
        åˆå§‹åŒ–çˆ¬èŸ²
        
        Args:
            config_path: è¨­å®šæª”è·¯å¾‘
        """
        self.base_url = "http://export.arxiv.org/api/query"
        self.config = self._load_config(config_path)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ArXiv-Daily-Summary/1.0 (https://github.com/audi0417/daily-arxiv-ai-summary)'
        })
        
    def _load_config(self, config_path: str) -> Dict:
        """è¼‰å…¥è¨­å®šæª”"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥è¨­å®šæª”: {config_path}")
            return config
        except FileNotFoundError:
            logger.warning(f"âš ï¸ è¨­å®šæª”ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é è¨­è¨­å®š")
            return self._get_default_config()
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥è¨­å®šæª”å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """å–å¾—é è¨­è¨­å®š"""
        return {
            'categories': ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL'],
            'limits': {
                'max_papers_per_day': 50,
                'max_papers_per_category': 10
            },
            'date_filter': {
                'recent_days': 3,
                'include_weekends': True
            },
            'keywords': {
                'include': [
                    'transformer', 'attention', 'deep learning', 
                    'neural network', 'machine learning'
                ]
            }
        }
    
    def _build_search_query(self, categories: List[str], date_from: str) -> str:
        """
        å»ºæ§‹ arXiv æœå°‹æŸ¥è©¢
        
        Args:
            categories: è«–æ–‡é¡åˆ¥åˆ—è¡¨
            date_from: èµ·å§‹æ—¥æœŸ (YYYYMMDD)
            
        Returns:
            æœå°‹æŸ¥è©¢å­—ä¸²
        """
        # å»ºæ§‹é¡åˆ¥æŸ¥è©¢
        cat_queries = [f"cat:{cat}" for cat in categories]
        cat_query = " OR ".join(cat_queries)
        
        # å»ºæ§‹æ—¥æœŸæŸ¥è©¢
        date_query = f"submittedDate:[{date_from}* TO *]"
        
        # çµåˆæŸ¥è©¢
        full_query = f"({cat_query}) AND {date_query}"
        
        logger.info(f"ğŸ” æœå°‹æŸ¥è©¢: {full_query}")
        return full_query
    
    def _parse_paper_entry(self, entry: ET.Element) -> Optional[Dict]:
        """
        è§£æå–®ç¯‡è«–æ–‡è³‡è¨Š
        
        Args:
            entry: XML entry å…ƒç´ 
            
        Returns:
            è«–æ–‡è³‡è¨Šå­—å…¸
        """
        try:
            # åŸºæœ¬è³‡è¨Š
            paper_id = entry.find('{http://www.w3.org/2005/Atom}id').text
            arxiv_id = paper_id.split('/')[-1]
            
            title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()
            title = re.sub(r'\s+', ' ', title)  # æ¸…ç†å¤šé¤˜ç©ºæ ¼
            
            summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()
            summary = re.sub(r'\s+', ' ', summary)  # æ¸…ç†å¤šé¤˜ç©ºæ ¼
            
            # ä½œè€…è³‡è¨Š
            authors = []
            for author in entry.findall('{http://www.w3.org/2005/Atom}author'):
                name = author.find('{http://www.w3.org/2005/Atom}name').text
                authors.append(name)
            
            # é¡åˆ¥è³‡è¨Š
            categories = []
            for cat in entry.findall('{http://arxiv.org/schemas/atom}primary_category'):
                categories.append(cat.get('term'))
            for cat in entry.findall('{http://arxiv.org/schemas/atom}category'):
                cat_term = cat.get('term')
                if cat_term not in categories:
                    categories.append(cat_term)
            
            # æ—¥æœŸè³‡è¨Š
            published_raw = entry.find('{http://www.w3.org/2005/Atom}published').text
            published = datetime.fromisoformat(published_raw.replace('Z', '+00:00'))
            
            updated_raw = entry.find('{http://www.w3.org/2005/Atom}updated').text
            updated = datetime.fromisoformat(updated_raw.replace('Z', '+00:00'))
            
            # PDF é€£çµ
            pdf_url = None
            for link in entry.findall('{http://www.w3.org/2005/Atom}link'):
                if link.get('title') == 'pdf':
                    pdf_url = link.get('href')
                    break
            
            return {
                'arxiv_id': arxiv_id,
                'title': title,
                'authors': authors,
                'summary': summary,
                'categories': categories,
                'published': published,
                'updated': updated,
                'pdf_url': pdf_url,
                'arxiv_url': f"https://arxiv.org/abs/{arxiv_id}"
            }
            
        except Exception as e:
            logger.error(f"âŒ è§£æè«–æ–‡è³‡è¨Šå¤±æ•—: {e}")
            return None
    
    def _filter_papers_by_keywords(self, papers: List[Dict]) -> List[Dict]:
        """
        æ ¹æ“šé—œéµå­—éæ¿¾è«–æ–‡
        
        Args:
            papers: è«–æ–‡åˆ—è¡¨
            
        Returns:
            éæ¿¾å¾Œçš„è«–æ–‡åˆ—è¡¨
        """
        if 'keywords' not in self.config:
            return papers
        
        keywords_config = self.config['keywords']
        include_keywords = keywords_config.get('include', [])
        exclude_keywords = keywords_config.get('exclude', [])
        
        filtered_papers = []
        
        for paper in papers:
            text_to_search = f"{paper['title']} {paper['summary']}".lower()
            
            # æª¢æŸ¥åŒ…å«é—œéµå­—
            if include_keywords:
                has_include_keyword = any(
                    keyword.lower() in text_to_search 
                    for keyword in include_keywords
                )
                if not has_include_keyword:
                    continue
            
            # æª¢æŸ¥æ’é™¤é—œéµå­—
            if exclude_keywords:
                has_exclude_keyword = any(
                    keyword.lower() in text_to_search 
                    for keyword in exclude_keywords
                )
                if has_exclude_keyword:
                    continue
            
            filtered_papers.append(paper)
        
        logger.info(f"ğŸ” é—œéµå­—éæ¿¾: {len(papers)} â†’ {len(filtered_papers)}")
        return filtered_papers
    
    def _apply_limits(self, papers: List[Dict]) -> List[Dict]:
        """
        æ‡‰ç”¨è«–æ–‡æ•¸é‡é™åˆ¶
        
        Args:
            papers: è«–æ–‡åˆ—è¡¨
            
        Returns:
            é™åˆ¶å¾Œçš„è«–æ–‡åˆ—è¡¨
        """
        limits = self.config.get('limits', {})
        max_papers = limits.get('max_papers_per_day', 50)
        
        if len(papers) > max_papers:
            # æŒ‰ç™¼å¸ƒæ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
            papers_sorted = sorted(papers, key=lambda x: x['published'], reverse=True)
            papers = papers_sorted[:max_papers]
            logger.info(f"ğŸ“Š æ‡‰ç”¨è«–æ–‡æ•¸é‡é™åˆ¶: {max_papers}")
        
        return papers
    
    def get_papers(self, target_date: Optional[str] = None) -> List[Dict]:
        """
        ç²å–æŒ‡å®šæ—¥æœŸçš„è«–æ–‡
        
        Args:
            target_date: ç›®æ¨™æ—¥æœŸ (YYYY-MM-DD)ï¼Œé è¨­ç‚ºä»Šæ—¥
            
        Returns:
            è«–æ–‡åˆ—è¡¨
        """
        if target_date is None:
            target_date = datetime.utcnow().strftime('%Y-%m-%d')
        
        # è¨ˆç®—æœå°‹æ—¥æœŸç¯„åœ
        target_dt = datetime.strptime(target_date, '%Y-%m-%d')
        recent_days = self.config.get('date_filter', {}).get('recent_days', 3)
        start_date = target_dt - timedelta(days=recent_days)
        date_from = start_date.strftime('%Y%m%d')
        
        logger.info(f"ğŸ” æœå°‹æ—¥æœŸç¯„åœ: {start_date.strftime('%Y-%m-%d')} åˆ° {target_date}")
        
        # å–å¾—é¡åˆ¥åˆ—è¡¨
        categories = self.config.get('categories', ['cs.AI', 'cs.LG'])
        logger.info(f"ğŸ“š æœå°‹é¡åˆ¥: {categories}")
        
        # å»ºæ§‹æœå°‹æŸ¥è©¢
        search_query = self._build_search_query(categories, date_from)
        
        # åŸ·è¡Œæœå°‹
        papers = self._search_papers(search_query)
        
        if not papers:
            logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•è«–æ–‡")
            return []
        
        logger.info(f"ğŸ“„ æ‰¾åˆ° {len(papers)} ç¯‡è«–æ–‡")
        
        # æ‡‰ç”¨éæ¿¾æ¢ä»¶
        papers = self._filter_papers_by_keywords(papers)
        papers = self._apply_limits(papers)
        
        logger.info(f"âœ… æœ€çµ‚ç²å¾— {len(papers)} ç¯‡è«–æ–‡")
        return papers
    
    def _search_papers(self, query: str, max_results: int = 100) -> List[Dict]:
        """
        åŸ·è¡Œ arXiv æœå°‹
        
        Args:
            query: æœå°‹æŸ¥è©¢
            max_results: æœ€å¤§çµæœæ•¸é‡
            
        Returns:
            è«–æ–‡åˆ—è¡¨
        """
        params = {
            'search_query': query,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        url = f"{self.base_url}?{urlencode(params)}"
        
        try:
            logger.info(f"ğŸŒ ç™¼é€è«‹æ±‚åˆ° arXiv API...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # è§£æ XML å›æ‡‰
            root = ET.fromstring(response.content)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
            for message in root.findall('.//{http://www.w3.org/2005/Atom}title'):
                if 'Error' in message.text:
                    logger.error(f"âŒ arXiv API éŒ¯èª¤: {message.text}")
                    return []
            
            # è§£æè«–æ–‡æ¢ç›®
            papers = []
            entries = root.findall('{http://www.w3.org/2005/Atom}entry')
            
            for entry in entries:
                paper = self._parse_paper_entry(entry)
                if paper:
                    papers.append(paper)
            
            # API è«‹æ±‚é™åˆ¶ï¼šæ¯ 3 ç§’æœ€å¤š 1 æ¬¡è«‹æ±‚
            time.sleep(3)
            
            return papers
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}")
            return []
        except ET.ParseError as e:
            logger.error(f"âŒ XML è§£æå¤±æ•—: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ æœå°‹è«–æ–‡æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")
            return []
    
    def get_paper_categories_stats(self, papers: List[Dict]) -> Dict[str, int]:
        """
        å–å¾—è«–æ–‡é¡åˆ¥çµ±è¨ˆ
        
        Args:
            papers: è«–æ–‡åˆ—è¡¨
            
        Returns:
            é¡åˆ¥çµ±è¨ˆå­—å…¸
        """
        stats = {}
        for paper in papers:
            for category in paper['categories']:
                stats[category] = stats.get(category, 0) + 1
        
        return dict(sorted(stats.items(), key=lambda x: x[1], reverse=True))
